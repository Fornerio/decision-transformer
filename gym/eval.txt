import os
import sys

import gym
import tqdm
import highway_env
from gym.wrappers import RecordVideo
import numpy as np
# from utils import RecordVideo
import torch
from modules.decision_transformer import DecisionTransformer
import pickle
# Evaluation
from highway_env.envs.common.evaluate import PrintMetrics
import random

ACTIONS_ALL = {0: 'LANE_LEFT',1: 'IDLE',2: 'LANE_RIGHT',3: 'FASTER',4: 'SLOWER'}
metricObj = PrintMetrics()

def make_highway_env():
    env = gym.make("dm-env-v0")
    env.unwrapped.configure({
        'observation': {
            'type': 'Kinematics',
            'vehicles_count': 7,
            'features': [
                'presence',
                'x',
                'y',
                'vx',
                'vy'
            ],
            'absolute': False
        },
        'duration': 50,
        'simulation_frequency': 15})

    print(env.observation_space)
    return env

# load training sequences
def load_sequence(row, collisions=True):
    '''
    Load a sequence from a row of the dataset.
    :param row: [[state1, action1, reward1], [state2, action2, reward2], ..., [stateN, actionN, rewardN]]
    :return: sequence: dict containing {'states': np.array([state1, state2, ..., stateT]),
                                       'actions': np.array([action1, action2, ..., actionT]),
                                       'rewards': np.array([reward1, reward2, ..., rewardT]),
                                       'dones': np.array([0,0, ..., 1])} -> trivial for our case as we always have one
                                       scene for each episode. Dones is also not used in experiments.
                    states: np.array of shape (T, *state_dim)
                    actions: np.array of shape (T, *action_dim)
                    rewards: np.array of shape (T, )
                    dones: np.array of shape (T, )
    '''
    states = []
    actions = []
    rewards = []
    for state, action, reward in row:
        # flatten state for mlp encoder
        states.append(state.reshape(-1))
        one_hot_action = np.zeros(3)
        one_hot_action[action] = 1
        actions.append(one_hot_action)
        rewards.append(reward)
    states = np.array(states)
    actions = np.array(actions)

    rewards = np.array(rewards)

    dones = np.zeros_like(rewards)
    dones[-1] = 1
    sequence = {'states': states, 'actions': actions, 'rewards': rewards, 'dones': dones}
    return sequence


# Load sequences
A = np.load(r'decision_transformer\datasets\50000_dataset.npy', allow_pickle=True)
sequences = [load_sequence(row) for row in A if len(row) > 10] #if row[-1] is False]

# get some useful statistics from training set
max_ep_len = max([len(path['states']) for path in sequences])  # take it as the longest trajectory

# load model
checkpoint = torch.load(r'decision_transformer\models\checkpoint-mlp-decision-transformer.pth', map_location='cpu', pickle_module=pickle)
render = True
config = {
    'device': 'cpu',#'cuda',
    'mode': 'normal',
    'experiment_name': 'mlp-decision-transformer',
    'group_name': 'ECE324',
    'log_to_wandb': False,
    'max_iters': ...,
    'num_steps_per_iter': 10000,#10000,
    'context_length': 15,
    'batch_size': ...,
    'num_eval_episodes': 100,
    'pct_traj': 1.0,
    'n_layer': 3,
    'embed_dim': 128,
    'n_head': 4,
    'activation_function': 'relu',
    'dropout': 0.1,
    'model': checkpoint['model'],
    'optimizer': None,
    'learning_rate': 1e-4,
    'warmup_steps': 10000,#10000,
    'weight_decay': 1e-4,
    'env_targets': [],#[0.5, 1.0, 5.0, 10],
    'action_tanh': False, #True,
    'loss_fn': lambda s_hat, a_hat, r_hat, s, a, r: torch.nn.CrossEntropyLoss()(a_hat, torch.argmax(a, dim=1)),
    #lambda s_hat, a_hat, r_hat, s, a, r: torch.mean((a_hat - a)**2),
    'err_fn': lambda a_hat, a: torch.sum(torch.argmax(a_hat, dim=1) != torch.argmax(a, dim=1))/a.shape[0],
}


env = make_highway_env()

state, done = env.reset(), False
state_dim = 35
act_dim = 3

model = DecisionTransformer(
        state_dim=state_dim,
        act_dim=act_dim,
        max_length=config['context_length'],
        max_ep_len=max_ep_len,
        action_tanh=config['action_tanh'],
        hidden_size=config['embed_dim'],
        n_layer=config['n_layer'],
        n_head=config['n_head'],
        n_inner=4 * config['embed_dim'],
        activation_function=config['activation_function'],
        n_positions=1024,
        resid_pdrop=config['dropout'],
        attn_pdrop=config['dropout'],
    )

model.load_state_dict(config['model'])

scale = np.mean([len(path['states']) for path in sequences])  # scale for rtg

# save all sequence information into separate lists
states, traj_lens, returns = [], [], []
for path in sequences:
    if config['mode'] == 'delayed':  # delayed: all rewards moved to end of trajectory
        path['rewards'][-1] = path['rewards'].sum()
        path['rewards'][:-1] = 0.
    states.append(path['states'])
    traj_lens.append(len(path['states']))
    returns.append(path['rewards'].sum())
traj_lens, returns = np.array(traj_lens), np.array(returns)

max_return = np.max(returns)

print('max_return: ',max_return)
print('mean_return: ',np.mean(returns))
print('min_return: ',np.min(returns))

# used for input normalization
states = np.concatenate(states, axis=0)
state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6
num_timesteps = sum(traj_lens)
print('num_timesteps: ',num_timesteps)


#highway_env.register_highway_envs()


model.eval()
model.to(device=config['device'])

state_mean = torch.from_numpy(state_mean).to(device=config['device'])
state_std = torch.from_numpy(state_std).to(device=config['device'])


# we keep all the histories on the device
# note that the latest action and reward will be "padding"
states = torch.from_numpy(state).reshape(1, state_dim).to(device=config['device'], dtype=torch.float32)
actions = torch.zeros((0, act_dim), device=config['device'], dtype=torch.float32)
rewards = torch.zeros(0, device=config['device'], dtype=torch.float32)

target_return = max_return

ep_return = target_return
target_return = torch.tensor(ep_return, device=config['device'], dtype=torch.float32).reshape(1, 1)
timesteps = torch.tensor(0, device=config['device'], dtype=torch.long).reshape(1, 1)

sim_states = []

num_episodes = 100

# Initialize totals
total_ep_length = 0
total_ep_return = 0

for episodes in tqdm.tqdm(range(num_episodes)):
    state = env.reset()
    done = False
    episode_return, episode_length = 0, 0
    total_rewards = []
    total_reward = 0

    decision_change_num, left_lane_change_num, right_lane_change_num, episode_reward = 0, 0, 0, 0
    accelerations, decelerations, speeds = [], [], []
    last_lane_idx = None
    last_action = ""

    t=0

    while not done:

        # add padding
        actions = torch.cat([actions, torch.zeros((1, act_dim), device=config['device'])], dim=0)
        rewards = torch.cat([rewards, torch.zeros(1, device=config['device'])])

        # num_embeddings = model.time_embeddings.num_embeddings  # Adjust this to match your model's attribute
        # max_valid_index = num_embeddings - 1

        # # Ensure timesteps are within the valid range
        # timesteps = torch.clamp(timesteps, 0, max_valid_index)

        action = model.get_action(
            (states.to(dtype=torch.float32) - state_mean) / state_std,
            actions.to(dtype=torch.float32),
            rewards.to(dtype=torch.float32),
            target_return.to(dtype=torch.float32),
            timesteps.to(dtype=torch.long),
        )
        actions[-1] = action
        action = action.detach().cpu().numpy()

        #print(ACTIONS_ALL[np.argmax(action)], action, t)
        #print(ACTIONS_ALL[np.argmax(action)], t)

        optimal_action = np.argmax(action)
        obs, reward, done, info  = env.step(optimal_action)

        if last_action != env.vehicle.current_action and last_action != "":
            decision_change_num += 1
        last_action = env.vehicle.current_action    
        
        speeds.append(env.vehicle.speed)

        if env.vehicle.throttle > 0:
            accelerations.append(env.vehicle.throttle)
        else:
            decelerations.append(env.vehicle.throttle)

        if last_lane_idx != env.vehicle.lane_index[2] and last_lane_idx is not None:
            if last_lane_idx > env.vehicle.lane_index[2]:
                left_lane_change_num += 1
            else:
                right_lane_change_num += 1
        last_lane_idx = env.vehicle.lane_index[2]

        cur_state = torch.from_numpy(state).to(device=config['device']).reshape(1, state_dim)
        states = torch.cat([states, cur_state], dim=0)
        rewards[-1] = reward

        if render:
            env.render()

        pred_return = target_return[0, -1]

        target_return = torch.cat(
            [target_return, pred_return.reshape(1, 1)], dim=1)

        timesteps = torch.cat(
            [timesteps,
             torch.ones((1, 1), device=config['device'], dtype=torch.long) * (t + 1)], dim=1)
        
        #print(timesteps)

        episode_return += reward
        t+=1

    episode_duration = (env.steps/env.config["policy_frequency"])
    left_lane_change_rate = left_lane_change_num / episode_duration
    right_lane_change_rate = right_lane_change_num / episode_duration
    mean_speed = np.mean(speeds)
    km_travelled = (mean_speed * episode_duration)/1000
    mean_acceleration = np.mean(accelerations)
    mean_deceleration = np.mean(decelerations)
    metricObj.saveEpisodeData(env_name='dm-env-v0', km_travelled=km_travelled, decision_change_num=decision_change_num, left_lane_change_num=left_lane_change_num,\
        right_lane_change_num=right_lane_change_num, mean_speed=mean_speed*3.6, mean_acceleration=mean_acceleration, mean_deceleration=mean_deceleration,\
        collision=env.vehicle.crashed, episode_duration=episode_duration, curr_episode_num=episodes+1)
    
    # Update episode metrics
#     ep_returns.append(episode_return)
#     ep_len.append(episode_length)
#     #mean_ep_speeds = np.mean(total_speed)
#     crash_counter += int(crashed)

#     # Update totals
#     total_ep_length += episode_length
#     total_ep_return += episode_return
#     #total_speeds.append(mean_ep_speeds)

# # Calculate averages and rates
# mean_ep_length = total_ep_length / num_episodes
# #mean_speed = np.mean(total_speeds) # Total speed divided by total lengths of episodes
# collision_rate = crash_counter / num_episodes
# mean_return = total_ep_return / num_episodes

# print(f"Mean Episode Length: {mean_ep_length}")
# #print(f"Mean Speed: {mean_speed}")
# print(f"Collision Rate: {collision_rate}")
# print(f"Mean Return: {mean_return}")
env.close()

csv_id = 'checkpoint-mlp-decision-transformer'
metricObj.printRecap(f"decision_transformer/models/", csv_id)
